Introduction

Le RAID (pour Redundant Array of Inexpensive Disks) est un syst√®me avanc√© de gestion du stockage utilis√© dans les entreprises qui mettent en place une politique de s√©curisation de leurs donn√©es.
Le RAID est un syst√®me qui utilise plusieurs disques durs physiques pour une gestion virtuelle des donn√©es.
Il existe 2 sortes de RAID : le RAID mat√©riel qui utilise des cartes contr√¥leurs qui se g√®re directement au niveau de la couche physique, et le RAID logiciel qui se cr√©er et se g√®re √† partir d'un syst√®me d'exploitation.
Plusieurs niveaux de RAID existent, de m√™me que plusieurs combinaisons. Suivant le niveau de RAID utilis√©, on am√©liore la performance, la fiabilit√© et/ou la capacit√© de stockage des donn√©es.
Dans cette qu√™te tu vas voir une rapide d√©finition des niveaux de RAID les plus utilis√©s avant d‚Äôexp√©rimenter du RAID 1.

La qu√™te suivante n'est pas un pr√©requis. N√©anmoins, la mani√®re d'op√©rer dans partie Simulation d'une panne reprend le m√™me principe pour Linux.

ü§ì Objectifs :

‚úÖ Comprendre les diff√©rents niveaux de RAID classiques
‚úÖ Manipuler des disques pour avoir du RAID 1
‚úÖ R√©parer un RAID 1 d√©fectueux
‚úÖ Simuler une panne de disque dur en ligne de commande
Sommaire

    üìå Glossaire
    üìå Les niveaux de RAID les plus courant
    üëâ Mise en ≈ìuvre
        üîß Pr√©requis
            ‚å®Ô∏è L'outil mdadm
        üî¨ Cr√©ation d'un RAID 1
            Partitionnement des disques
            Cr√©ation du RAID
            V√©rification
            Formatage du RAID
            Montage du RAID
            Verrouillage du nom md0 de la partition RAID
        üîß Test du bon fonctionnement du RAID 1
        üîß Simulation d'une panne
            V√©rification de la tol√©rance aux pannes
            Reconstruction du RAID
    üìù Quiz
    ‚òùÔ∏è R√©sum√©
    üí™ Challenge
    üßê Crit√®res d'acceptation

üìå Glossaire

Des termes sp√©cifiques sont utilis√©s dans le contexte du stockage. Tu as ci-dessous quelques d√©finitions :

    Un disque dur (HDD) ou un SSD est un p√©riph√©rique de stockage de donn√©es aussi appel√© p√©riph√©rique de blocs (block device).

    Un bloc est une unit√© de stockage de donn√©es de petite taille (de 512 octets ou 4096 octets). Les donn√©es sont souvent stock√©es en plusieurs blocs sur un disque dur.

    Une grappe (ou cluster ou array) est une collection de disques durs reli√©s ensemble.

    Un calcul de parit√© est une technique utilis√©e pour prot√©ger les donn√©es en cas de d√©faillance d'un disque dur. C'est une information suppl√©mentaire stock√©e sur un ou plusieurs disques durs qui peut √™tre utilis√©e pour reconstruire les donn√©es en cas de d√©faillance d'un disque.

üìå Les niveaux de RAID les plus courant

De nombreux niveaux de RAID existent, certains sont plus utilis√©s que d'autres. En voici donc quelques un :

    Le RAID 0 (entrela√ßage par bande ou striping) :
    Les donn√©es sont divis√©es en blocs et stock√©es sur diff√©rents disques. Cela am√©liore les performances en raison de la lecture et de l'√©criture simultan√©es sur plusieurs disques, mais ne fournit pas de redondance. Si l'un des disques est en panne, toutes les donn√©es sont perdues.

sch√©ma du RAID 0 stripping

    Le RAID 1 (disques mirroirs ou mirroring) :
    Les donn√©es sont stock√©es en double sur deux disques. Si l'un des disques est en panne, les donn√©es peuvent √™tre lues √† partir de l'autre disque. Ce niveau de RAID fournit une tr√®s haute fiabilit√©, mais ne fournit pas un gain de performance significatif.

sch√©ma du RAID 1 mirroring

    Le RAID 5 :
    Ce niveau implique l'utilisation d'au moins 3 disques. Les donn√©es sont divis√©es en blocs et stock√©es sur diff√©rents disques, tandis que des informations de parit√© sont stock√©es (de mani√®re tournante) sur un autre disque. Si l'un des disques dysfonctionne, les informations de parit√© peuvent √™tre utilis√©es pour reconstruire les donn√©es perdue sur un nouveau disque.

sch√©ma du RAID 5

    Le RAID 1+0 ou RAID 10 :
    Cette combinaison de RAID 1 et 0, appel√© plus commun√©ment RAID 10 combine le striping et le mirroring. Les donn√©es sont d'abord mises en miroir sur deux groupes de disques (RAID 1), puis divis√©es en blocs et stock√©es sur plusieurs disques dans chaque groupe (RAID 0). Ce niveau de RAID fournit √† la fois de hautes performances et une fiabilit√© √©lev√©e.
    En effet, en cas de d√©faillance d'un disque dans un groupe, les donn√©es peuvent √™tre lues √† partir du miroir gr√¢ce au RAID 1. De plus, le RAID 0 offre, avec la duplication des blocs de tr√®s bonnes performances de lecture.

sch√©ma du RAID 10

Le RAID sur Wikip√©diA

Pour en apprendre un peu plus sur le RAID, tu peux consulter cette page Wikip√©diA
https://fr.wikipedia.org/wiki/RAID_(informatique)

La vid√©o suivante constitue une bonne illustration des diff√©rents types de RAID.

üëâ Mise en ≈ìuvre
üîß Pr√©requis

Pour pouvoir exp√©rimenter sereinement, il est pr√©f√©rable de travailler sur une machine d√©di√©e au test.
Il est ainsi possible de la red√©marrer, reconfigurer voir casser sans impact sur les utilisateurs.
Une strat√©gie courante est de s'appuyer sur des machines virtuelles avec des disques virtuels.

Les exp√©rimentations pratiques ont √©t√© test√©es sur une distribution Linux Ubuntu 22.04 LTS install√©e dans une machine virtuelle VirtualBox 7 tournant sur un syst√®me h√¥te Ubuntu 22.04 LTS.
Elles peuvent √™tre reproduites avec d'autres distributions Linux, sur d'autres environnement, mais des diff√©rences peuvent alors appara√Ætre.

La machine de test est une VM avec la configuration suivante :

    OS : Linux Ubuntu 22.04 LTS
    Disque 1 : pour le syst√®me
    3 disques suppl√©mentaires de 5 Go chacun pour les manipulations sur le RAID. Tu peux changer la taille mais fait en sorte qu'elle soit identique pour tous les disques.
    Attention √† bien cocher la case pour que les disques soit branchable √† chaud (ou hotplug).

Pour du RAID 1, les disques peuvent avoir des tailles diff√©rentes, mais la taille du plus petit des disques limite la taille de la grappe.
Par exemple, pour du RAID 1 avec un disque de 100 Go et un disque de 500 Go, la taille du volume du RAID sera de 100 Go.

‚å®Ô∏è L'outil mdadm

Pour administrer le RAID logiciel sous GNU/Linux, on utilise l'outil mdadm. S'il n'est pas install√© sur ton syst√®me, ex√©cute la commande apt install mdadm pour l'installer.

Mdadm

La page Wikipedia de cet outil de gestion.
https://en.wikipedia.org/wiki/Mdadm

üî¨ Cr√©ation d'un RAID 1

L'objectif ici est de mettre en place un RAID 1, soit du mirroring entre les 2 disques. Les donn√©es sont copi√©es simultan√©ment sur deux disques. Ainsi en cas de panne d'un des disques, les donn√©es sont toujours disponibles sur le disque restant.

Une fois la grappe RAID cr√©√©e, elle appara√Æt sous la forme d'un unique p√©riph√©rique de blocs suppl√©mentaire. C'est ce qui rend le RAID transparent pour les applications qui continuent √† manipuler l'√©quivalent d'un disque.

Partitionnement des disques

Avec la commande lsblk on peut voir les 3 disques qui vont servir au RAID :

wilder@Ubuntu:~$ lsblk

NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS


[...]

                                 /

sdb      8:16   0     5G  0 disk 

sdc      8:32   0     5G  0 disk 

sdd      8:48   0     5G  0 disk 


[...]

    On voit bien que les 3 disques font 5 Go chacun

Avec la commande fdisk -l on a plus de d√©tails :

wilder@Ubuntu:~$ sudo fdisk -l


[...]


Disque /dev/sdb : 5 GiB, 5368709120 octets, 10485760 secteurs

Disk model: VBOX HARDDISK   

Unit√©s : secteur de 1 √ó 512 = 512 octets

Taille de secteur (logique / physique) : 512 octets / 512 octets

taille d'E/S (minimale / optimale) : 512 octets / 512 octets



Disque /dev/sdc : 5 GiB, 5368709120 octets, 10485760 secteurs

Disk model: VBOX HARDDISK   

Unit√©s : secteur de 1 √ó 512 = 512 octets

Taille de secteur (logique / physique) : 512 octets / 512 octets

taille d'E/S (minimale / optimale) : 512 octets / 512 octets



Disque /dev/sdd : 5 GiB, 5368709120 octets, 10485760 secteurs

Disk model: VBOX HARDDISK   

Unit√©s : secteur de 1 √ó 512 = 512 octets

Taille de secteur (logique / physique) : 512 octets / 512 octets

taille d'E/S (minimale / optimale) : 512 octets / 512 octets


[...]

Partitionner le disque sdb avec la commande fdisk /dev/sdb. Cela ouvre l'utilitaire de partitionnement.
En choisissant la commande m dans l'utilitaire, l'aide va appara√Ætre :

Commande (m pour l'aide) : m

Aide :

  DOS (secteur d'amor√ßage)
   a   modifier un indicateur d'amor√ßage
   b   √©diter l'√©tiquette BSD imbriqu√©e du disque
   c   modifier l'indicateur de compatibilit√© DOS

  G√©n√©rique
   d   supprimer la partition
   F   afficher l‚Äôespace libre non partitionn√©
   l   afficher les types de partitions connues
   n   ajouter une nouvelle partition
   p   afficher la table de partitions
   t   modifier le type d'une partition
   v   v√©rifier la table de partitions
   i   Afficher des renseignements sur la partition

  Autre
   m   afficher ce menu
   u   modifier les unit√©s d'affichage et de saisie
   x   fonctions avanc√©es (r√©serv√©es aux sp√©cialistes)

  Script
   I   chargement de l‚Äôagencement √† partir du fichier de script sfdisk
   O   sauvegarde de l‚Äôagencement vers le fichier de script sfdisk

  Sauvegarder et quitter
   w   √©crire la table sur le disque et quitter
   q   quitter sans enregistrer les modifications

  Cr√©er une nouvelle √©tiquette
   g   cr√©er une nouvelle table vide de partitions GPT
   G   cr√©er une nouvelle table vide de partitions SGI (IRIX)
   o   cr√©er une nouvelle table vide de partitions DOS
   s   cr√©er une nouvelle table vide de partitions Sun

En te guidant avec cet utilitaire, ajoute une nouvelle partition (avec n) et cr√©er une partition primaire qui prend la totalit√© du disque. Ensuite, modifie le type de la partition en RAID Linux
auto.
Quand tu as fait toutes ces actions, sauvegarde et quitte l'utilitaire.
V√©rifie avec la commande lsblk que le disque est bien partitionn√©.
Si c'est le cas, tu as l'information comme ceci :

P√©riph√©rique Amor√ßage D√©but      Fin Secteurs Taille Id Type

/dev/sdb1              2048 10485759 10483712     5G fd RAID Linux autod√©tect√©

Fais la m√™me chose avec le 2√®me disque /dev/sdc.

Avec lsblk tu dois avoir ceci :

wilder@Ubuntu:~$ lsblk

NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS


[...]

                            /

sdb      8:16   0     5G  0 disk 

‚îî‚îÄsdb1   8:17   0     5G  0 part 

sdc      8:32   0     5G  0 disk 

‚îî‚îÄsdc1   8:33   0     5G  0 part 

sdd      8:48   0     5G  0 disk 

    Tu vois bien chaque partition sdb1 et sdc1.

Cr√©ation du RAID

Ex√©cute la commande sudo mdadm --create /dev/md0 --level 1 --raid-devices 2 /dev/sdb1 /dev/sdc1

Explications de cette commande :

    /dev/md0 : nom du p√©riph√©rique RAID √† cr√©er
    --level 1 : niveau de RAID, soit ici du RAID 1 (on peut aussi √©crire --level=1)
    --raid-devices 2 : le nombre de disques, soit ici 2 (on peut aussi √©crire --raid-devices=2)
    /dev/sdb1 /dev/sdc1 : les partitions des disques concern√©s

Voici le r√©sultat :

wilder@Ubuntu:~$ sudo mdadm --create /dev/md0 --level 1 --raid-devices 2 /dev/sdb1 /dev/sdc1

mdadm: Note: this array has metadata at the start and

    may not be suitable as a boot device.  If you plan to

    store '/boot' on this device please ensure that

    your boot-loader understands md/v1.x metadata, or use

    --metadata=0.90

Continue creating array? y

mdadm: Defaulting to version 1.2 metadata

mdadm: array /dev/md0 started.

V√©rification

Pour v√©rifier l'√©tat du RAID, il faut aller voir le contenu du fichier mdstat :

wilder@Ubuntu:~$ cat /proc/mdstat

Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 

md0 : active raid1 sdb1[0] sdc1[1]

      5236736 blocks super 1.2 [2/2] [UU]


      

unused devices: <none>

Le r√©sultat de la commande donne un RAID 1 actif md0 avec les partitions sdb1 et sdc1.
De plus[UU] indique que les 2 disques sont en marche (Up).

Aller plus loin avec mdstat

Tous les d√©tails sur ce fichier.
https://raid.wiki.kernel.org/index.php/Mdstat

Tu peux aussi voir l'√©tat du RAID avec la commande mdadm :

wilder@Ubuntu:~$ sudo mdadm --detail /dev/md0

/dev/md0:

           Version : 1.2

     Creation Time : Wed Feb 08 10:41:03 2023

        Raid Level : raid1

        Array Size : 5236736 (4.99 GiB 5.36 GB)

     Used Dev Size : 5236736 (4.99 GiB 5.36 GB)

      Raid Devices : 2

     Total Devices : 2

       Persistence : Superblock is persistent


       Update Time : Wed Feb 08 10:41:30 2023

             State : clean 

    Active Devices : 2

   Working Devices : 2

    Failed Devices : 0

     Spare Devices : 0


Consistency Policy : resync


              Name : Ubuntu:0  (local to host Ubuntu)

              UUID : 79dcf1ad:b0244cd7:cf2aced4:f0e99d3b

            Events : 17


    Number   Major   Minor   RaidDevice State

       0       8       17        0      active sync   /dev/sdb1

       1       8       33        1      active sync   /dev/sdc1

    Ici le statut du RAID est clean et les partitions concern√©es sont bien /dev/sdb1 et /dev/sdc1.
    De plus, /dev/sdb1 et /dev/sdc1 sont bien synchronis√©s.

On peut aussi voir les disques inclus dans le RAID md0 avec la commande lsblk -f.

wilder@Ubuntu:~$ lsblk -f

NAME    FSTYPE            FSVER LABEL          UUID                                 FSAVAIL FSUSE% MOUNTPOINTS


[...]

                                                                                                   /

sdb                                                                                                

‚îî‚îÄsdb1  linux_raid_member 1.2   Ubuntu:0 79dcf1ad-b024-4cd7-cf2a-ced4f0e99d3b                

  ‚îî‚îÄmd0                                                                                            

sdc                                                                                                

‚îî‚îÄsdc1  linux_raid_member 1.2   Ubuntu:0 79dcf1ad-b024-4cd7-cf2a-ced4f0e99d3b                

  ‚îî‚îÄmd0                                                                                            

sdd

Formatage du RAID

Formates le volume RAID md0 avec un file system en ext4 et avec le nom PersonalData.

wilder@Ubuntu:~$ sudo mkfs.ext4 /dev/md0 -L "PersonalData"

mke2fs 1.46.5 (30-Dec-2021)

En train de cr√©er un syst√®me de fichiers avec 1309184 4k blocs et 327680 i-noeuds.

UUID de syst√®me de fichiers=6446c9f7-5d25-43f4-bc94-18a09c7dd9b8

Superblocs de secours stock√©s sur les blocs : 

	32768, 98304, 163840, 229376, 294912, 819200, 884736


Allocation des tables de groupe : compl√©t√©                            

√âcriture des tables d'i-noeuds : compl√©t√©                            

Cr√©ation du journal (16384 blocs) : compl√©t√©

√âcriture des superblocs et de l'information de comptabilit√© du syst√®me de

fichiers : compl√©t√©

Si tu relances la commande lsblk -f, tu vois que md0 est format√© en ext4 et que le nom est PersonalData :

wilder@Ubuntu:~$ lsblk -f

NAME   FSTYPE            FSVER LABEL    UUID                                 FSAVAIL FSUSE% MOUNTPOINTS

[...]

sdb                                                                                                

‚îî‚îÄsdb1  linux_raid_member 1.2   Ubuntu:0 79dcf1ad-b024-4cd7-cf2a-ced4f0e99d3b                

  ‚îî‚îÄmd0 ext4              1.0   PersonalData   6446c9f7-5d25-43f4-bc94-18a09c7dd9b8                

sdc                                                                                                

‚îî‚îÄsdc1  linux_raid_member 1.2   Ubuntu:0 79dcf1ad-b024-4cd7-cf2a-ced4f0e99d3b                

  ‚îî‚îÄmd0 ext4              1.0   PersonalData   6446c9f7-5d25-43f4-bc94-18a09c7dd9b8                

sdd   

[...]                                                                                                                           

Montage du RAID

Ajoute un dossier Data-Raid1 sous /home/wilder, pour monter la partition md0 :

wilder@Ubuntu:~$ sudo mkdir /home/wilder/Data-RAID1 -p

Montes ensuite la partition md0 dans ce dossier :

wilder@Ubuntu:~$ sudo mount /dev/md0 /home/wilder/Data-RAID1/

Pour que le RAID soit mont√© automatiquement √† chaque d√©marrage, il faut ajouter la ligne suivante au fichier /etc/fstab :

/dev/md0 /home/wilder/Data-RAID1 ext4 nofail 0 0

Qu'indique cette ligne :

    /dev/md0 : Le RAID
    /home/wilder/Data-RAID1 : Le point de montage de la partition RAID
    ext4 : Le FS choisi pour le RAID
    nofail : Avec cette option, il n'y aura pas de blocage au boot s'il y a un probl√®me avec md0.

En g√©neral, on met default, mais ici on met nofail pour √©viter d'√™tre bloqu√© au d√©marrage de la machine.

    0 : C'est un champ pour les options de sauvegarde. La valeur 0 signifie que le syst√®me de fichiers ne sera pas sauvegard√© lors de l'ex√©cution de la commande dump.
    0 : C'est un champ pour la fr√©quence de v√©rification du syst√®me de fichiers. La valeur 0 signifie que le syst√®me de fichiers ne sera pas v√©rifi√© lors du red√©marrage.

Verrouillage du nom md0 de la partition RAID

La commande sudo mdadm --detail --scan donne le r√©sultat suivant :

wilder@Ubuntu:~$ sudo mdadm --detail --scan

ARRAY /dev/md0 metadata=1.2 name=Ubuntu:0 UUID=79dcf1ad:b0244cd7:cf2aced4:f0e99d3b

Erreur de montage de RAID due √† un mauvais nom de partition

Sans verrouillage du nom de la partition RAID, le syst√®me va changer le nom. Pour √©viter cela, il faut utiliser l'UUID du p√©riph√©rique. Sinon le nom md0 va se transformer en mdX avec X commen√ßant √† 127.
Un RAID qui n'est pas bien configur√© et qui est mal mont√© au boot peut emp√™cher le syst√®me de d√©marrer.

Pour √©viter la modification du nom de la partition RAID, ajouter la ligne ARRAY /dev/md0 metadata=1.2 name=Ubuntu:0 UUID=79dcf1ad:b0244cd7:cf2aced4:f0e99d3b au fichier /etc/mdadm/mdadm.conf
pour lier le nom md0 √† son UUID.

En faisant cela, /etc/fstab va v√©rifier dans /etc/mdadm/mdadm.conf le lien entre le nom et l'UUID.
On peut aussi mettre directement l'UUID du p√©riph√©rique dans /etc/fstab sous la forme :
UUID=79dcf1ad:b0244cd7:cf2aced4:f0e99d3b /media/wilder/Data-RAID1 ext4 nofail 0 0

Les UUIDs

Comment bien les utiliser dans fstab.
https://doc.ubuntu-fr.org/mount_fstab#note_sur_les_uuids

Forcer la mise en application des changements avec la commande sudo update-initramfs -u et red√©marrer la machine.

Documentation sur la commande update-initramfs

La documentation pour Ubuntu 22.04 et pour les autres versions.
https://manpages.ubuntu.com/manpages/jammy/en/man8/update-initramfs.8.html

üîß Test du bon fonctionnement du RAID 1

Cr√©er un fichier ou un dossier sur la partition md0 qui est accessible directement depuis le home.

Les droits d'acc√®s √† ce nouveau syst√®me de fichier doivent probablement √™tre param√©tr√©s correctement en fonction des utilisateurs qui doivent pouvoir y acc√©der. Les commandes chmod et chown peuvent t'y aider.

üîß Simulation d'une panne

Test cette possibilit√© en retirant un des 2 disques.

    Sur VirtualBox cette manipulation peut-√™tre faite via l'interface graphique (Option Stockage dans la Configuration de la VM), √† condition d'√©teindre la machine. En effet, l'interface graphique permet de choisir la configuration au d√©marrage de la VM.

    Il est aussi possible de supprimer un disque en ligne (c'est √† dire sans √©teindre la VM) avec la ligne de commande VirtualBox.

    La commande vboxmanage showvminfo permet d'obtenir toutes les informations concernant une VM.
    Les informations concernant le Storage Controller utilis√© ainsi que le port et l'identifiant UUID de chaque disque virtuel sont n√©cessaires pour la commande suivante.

wilder@ubuntu:~$ vboxmanage showvminfo "Ubuntu"

Name:                        Ubuntu

Groups:                      /

Guest OS:                    Ubuntu (64-bit)


[...]

Storage Controllers:

#0: 'IDE', Type: PIIX4, Instance: 0, Ports: 2 (max 2), Bootable

  Port 1, Unit 0: Empty

#1: 'SATA', Type: IntelAhci, Instance: 0, Ports: 4 (max 30), Bootable

  Port 0, Unit 0: UUID: 8cafd925-d47b-4c95-9d19-17bdf923563c

    Location: "/home/wilder/VirtualBox VMs/Ubuntu/Ubuntu.vmdk"

  Port 1, Unit 0: UUID: 253d36da-efb1-4394-9727-9433aeffad63, hot-pluggable

    Location: "/home/wilder/VirtualBox VMs/Ubuntu/Ubuntu_1.vmdk"

  Port 2, Unit 0: UUID: 4c19f253-d875-445e-bc8d-b50b38016269, hot-pluggable

    Location: "/home/wilder/VirtualBox VMs/Ubuntu/Ubuntu_2.vmdk"

  Port 3, Unit 0: UUID: b412c57c-fd21-4575-b819-874143a4c9a9, hot-pluggable

    Location: "/home/wilder/VirtualBox VMs/Ubuntu/Ubuntu_3.vmdk"

[...]

    On voit ainsi que cette VM, nomm√©e Ubuntu, dispose de 2 contr√¥leurs de disques

        Un contr√¥leur 0 nomm√© IDE
        Un contr√¥leur 1 nomm√© SATA

    Le contr√¥leur SATA a 4 ports sur lesquels on trouve un disque :

        Un disque d'UUID 8cafd925-d47b-4c95-9d19-17bdf923563c sur le port 0
        Un disque d'UUID 253d36da-efb1-4394-9727-9433aeffad63 sur le port 1
        Un disque d'UUID 4c19f253-d875-445e-bc8d-b50b38016269 sur le port 2
        Un disque d'UUID b412c57c-fd21-4575-b819-874143a4c9a9 sur le port 3

    Chaque port du contr√¥leur n'est connect√© qu'√† 1 disque, mais il pourrait y en avoir plusieurs sur certains contr√¥leurs, ce qui explique le 0 dans Port X, Unit 0.

    Muni de ces informations, il est possible de connecter ou d√©connecter des disques avec la commande vboxmanage storageattach

    Par exemple, pour d√©connecter le dernier des 4 disques pr√©c√©dents, sur le port 3 donc, on indique √† VirtualBox qu'on souhaite le remplacer par un medium : none avec la commande suivante :


wilder@ubuntu:~$ vboxmanage storageattach "Ubuntu" --storagectl SATA --port 3 --medium none

    Pour reconnecter le disque, on utilise la m√™me commande, toujours en pr√©cisant la VM, le contr√¥leur disque et le port, mais en pr√©cisant en plus le type hdd et le UUID du disque en medium.


wilder@ubuntu:~$ VBoxManage storageattach "Ubuntu" --storagectl SATA --port 3 --type hdd --medium b412c57c-fd21-4575-b819-874143a4c9a9

La documentation VirtualBox

Plus de d√©tail sur l'utilisation de VirtualBox en ligne de commande sur la doc officielle.
https://www.virtualbox.org/manual/ch08.html#vboxmanage-storageattach

Pour enlever un disque, tu peux aussi utiliser la commande sudo umount /dev/sdX directement dans la VM Ubuntu.

V√©rification de la tol√©rance aux pannes

Une fois le disque retir√©, v√©rifie que ton fichier est toujours pr√©sent, que tu peux y acc√©der et m√™me le modifier ou ajouter d'autres fichiers.

On voit dans le r√©sultat de la commande mdadm --detail /dev/md0 que la situation n'est pas normale car l'√©tat du RAID est Degraded et il y a un disque manquant.

En effet, la perte du second disque entra√Ænerait cette fois-ci la perte des donn√©es.
Reconstruction du RAID

On va utiliser le 3√®me disque pr√©vu pour le RAID, /dev/sdd qui a √©t√© mis de cot√© pour l'instant.
Comme pour les 2 autres disques sdb et sdc, cr√©er une partition principal de la taille du disque, et de type RAID Linux auto pour pouvoir l'ajouter au RAID.

Pour ajouter ce disque au RAID existant md0, ex√©cute la commande suivante :

wilder@Ubuntu:~$ sudo mdadm --manage /dev/md0 --add /dev/sdd1

mdadm: added /dev/sdd1

wilder@Ubuntu:~$

Tu peux v√©rifier avec la commande sudo mdadm --detail /dev/md0 que le RAID est reconstruit.

√Ä noter que :

    Un moment de reconstruction est n√©cessaire √† la recopie des informations sur l'autre disque. Ce temps peut √™tre tr√®s long dans le cas d'un disque concernant beaucoup de donn√©es.

‚òùÔ∏è R√©sum√©

Tu sais maintenant configurer des volumes RAID sur une distribution Linux Ubuntu.
Cette vid√©o explique pas-√†-pas un exercice presque identique.

üí™ Challenge

Dans un premier temps, ex√©cute les diff√©rentes actions de cette qu√™te :

    Cr√©er un RAID 1 avec 2 disques
    Reconstruire le RAID apr√®s une simulation de panne
    Une fois que tout est fonctionnel, refait-l√†, mais cette fois-ci avec un autre type de RAID, comme le RAID 5 par exemple.

üßê Crit√®res d'acceptation

    Cr√©ation d'un RAID 1 fonctionnel
    Cr√©ation d'un autre type de RAID fonctionnel
    Historique de la commande history (filtr√©es des commandes inutiles et des erreurs...) mis en solution.

